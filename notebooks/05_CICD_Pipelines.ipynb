{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CI/CD for Microsoft Fabric\n",
    "\n",
    "This notebook demonstrates implementing Continuous Integration and Continuous Deployment (CI/CD) practices for Microsoft Fabric solutions.\n",
    "\n",
    "## What is CI/CD for Fabric?\n",
    "\n",
    "CI/CD for Fabric enables:\n",
    "- **Version Control**: Track changes to Fabric items using Git\n",
    "- **Automated Testing**: Validate changes before deployment\n",
    "- **Deployment Automation**: Push changes across environments\n",
    "- **Collaboration**: Enable team-based development\n",
    "- **Rollback Capabilities**: Quickly revert problematic changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "To implement CI/CD for Fabric, you need:\n",
    "- Azure DevOps or GitHub repository\n",
    "- Multiple Fabric workspaces (Dev, Test, Prod)\n",
    "- Fabric capacity with Git integration enabled\n",
    "- Service Principal or managed identity\n",
    "- Appropriate permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Git Integration Setup\n",
    "\n",
    "### Connecting Fabric to Git:\n",
    "1. Navigate to Workspace settings\n",
    "2. Select \"Git integration\"\n",
    "3. Connect to Azure DevOps or GitHub\n",
    "4. Select repository and branch\n",
    "5. Configure folder structure\n",
    "\n",
    "### Supported Items:\n",
    "- Notebooks\n",
    "- Lakehouses\n",
    "- Data Pipelines\n",
    "- Semantic Models\n",
    "- Reports\n",
    "- Dataflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fabric Item Structure in Git\n",
    "\n",
    "Understanding how Fabric items are stored in Git:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Example folder structure for Fabric items in Git\n",
    "fabric_git_structure = \"\"\"\n",
    "Repository/\n",
    "├── .fabric/\n",
    "│   └── config.json\n",
    "├── Notebooks/\n",
    "│   ├── DataTransform.Notebook/\n",
    "│   │   ├── notebook-content.py\n",
    "│   │   └── .platform\n",
    "│   └── Analysis.Notebook/\n",
    "│       ├── notebook-content.py\n",
    "│       └── .platform\n",
    "├── DataPipelines/\n",
    "│   └── ETL_Pipeline.DataPipeline/\n",
    "│       ├── pipeline-content.json\n",
    "│       └── .platform\n",
    "├── Lakehouses/\n",
    "│   └── SalesLakehouse.Lakehouse/\n",
    "│       ├── lakehouse-content.json\n",
    "│       └── .platform\n",
    "├── SemanticModels/\n",
    "│   └── SalesModel.SemanticModel/\n",
    "│       ├── model.bim\n",
    "│       └── .platform\n",
    "└── Reports/\n",
    "    └── SalesDashboard.Report/\n",
    "        ├── report.json\n",
    "        └── .platform\n",
    "\"\"\"\n",
    "\n",
    "print(\"Fabric Git Repository Structure:\")\n",
    "print(fabric_git_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Environment Configuration\n",
    "\n",
    "Managing different environments (Dev, Test, Prod):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment configuration file example\n",
    "environments = {\n",
    "    \"development\": {\n",
    "        \"workspace_id\": \"dev-workspace-guid\",\n",
    "        \"capacity_name\": \"dev-capacity\",\n",
    "        \"lakehouse_name\": \"dev_lakehouse\",\n",
    "        \"storage_account\": \"devstorageaccount\",\n",
    "        \"key_vault\": \"dev-keyvault\",\n",
    "        \"service_principal_id\": \"dev-sp-id\",\n",
    "        \"tenant_id\": \"your-tenant-id\"\n",
    "    },\n",
    "    \"test\": {\n",
    "        \"workspace_id\": \"test-workspace-guid\",\n",
    "        \"capacity_name\": \"test-capacity\",\n",
    "        \"lakehouse_name\": \"test_lakehouse\",\n",
    "        \"storage_account\": \"teststorageaccount\",\n",
    "        \"key_vault\": \"test-keyvault\",\n",
    "        \"service_principal_id\": \"test-sp-id\",\n",
    "        \"tenant_id\": \"your-tenant-id\"\n",
    "    },\n",
    "    \"production\": {\n",
    "        \"workspace_id\": \"prod-workspace-guid\",\n",
    "        \"capacity_name\": \"prod-capacity\",\n",
    "        \"lakehouse_name\": \"prod_lakehouse\",\n",
    "        \"storage_account\": \"prodstorageaccount\",\n",
    "        \"key_vault\": \"prod-keyvault\",\n",
    "        \"service_principal_id\": \"prod-sp-id\",\n",
    "        \"tenant_id\": \"your-tenant-id\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Environment Configurations:\")\n",
    "print(json.dumps(environments, indent=2))\n",
    "\n",
    "# Save as JSON file\n",
    "os.makedirs('/tmp/cicd_config', exist_ok=True)\n",
    "with open('/tmp/cicd_config/environments.json', 'w') as f:\n",
    "    json.dump(environments, f, indent=2)\n",
    "print(\"\\n✓ Configuration saved to /tmp/cicd_config/environments.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Azure DevOps Pipeline (YAML)\n",
    "\n",
    "Example CI/CD pipeline using Azure DevOps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure DevOps Pipeline YAML\n",
    "azure_devops_pipeline = \"\"\"\n",
    "# azure-pipelines.yml\n",
    "trigger:\n",
    "  branches:\n",
    "    include:\n",
    "    - main\n",
    "    - develop\n",
    "  paths:\n",
    "    include:\n",
    "    - Notebooks/*\n",
    "    - DataPipelines/*\n",
    "    - SemanticModels/*\n",
    "\n",
    "variables:\n",
    "  - group: fabric-dev-variables\n",
    "  - name: pythonVersion\n",
    "    value: '3.10'\n",
    "\n",
    "stages:\n",
    "- stage: Build\n",
    "  displayName: 'Build and Validate'\n",
    "  jobs:\n",
    "  - job: Validate\n",
    "    displayName: 'Validate Fabric Items'\n",
    "    pool:\n",
    "      vmImage: 'ubuntu-latest'\n",
    "    steps:\n",
    "    - task: UsePythonVersion@0\n",
    "      inputs:\n",
    "        versionSpec: $(pythonVersion)\n",
    "      displayName: 'Use Python $(pythonVersion)'\n",
    "    \n",
    "    - script: |\n",
    "        pip install pytest nbformat nbconvert semantic-link-labs\n",
    "      displayName: 'Install dependencies'\n",
    "    \n",
    "    - script: |\n",
    "        python scripts/validate_notebooks.py\n",
    "      displayName: 'Validate Notebooks'\n",
    "    \n",
    "    - script: |\n",
    "        python scripts/validate_pipelines.py\n",
    "      displayName: 'Validate Pipelines'\n",
    "    \n",
    "    - task: PublishTestResults@2\n",
    "      inputs:\n",
    "        testResultsFiles: '**/test-results.xml'\n",
    "      displayName: 'Publish Test Results'\n",
    "\n",
    "- stage: DeployDev\n",
    "  displayName: 'Deploy to Dev'\n",
    "  dependsOn: Build\n",
    "  condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/develop'))\n",
    "  jobs:\n",
    "  - deployment: DeployToDev\n",
    "    displayName: 'Deploy to Dev Workspace'\n",
    "    environment: 'fabric-dev'\n",
    "    pool:\n",
    "      vmImage: 'ubuntu-latest'\n",
    "    strategy:\n",
    "      runOnce:\n",
    "        deploy:\n",
    "          steps:\n",
    "          - checkout: self\n",
    "          \n",
    "          - task: AzureCLI@2\n",
    "            inputs:\n",
    "              azureSubscription: 'fabric-service-connection'\n",
    "              scriptType: 'bash'\n",
    "              scriptLocation: 'inlineScript'\n",
    "              inlineScript: |\n",
    "                echo \"Deploying to Dev workspace...\"\n",
    "                python scripts/deploy_to_fabric.py --environment dev\n",
    "            displayName: 'Deploy to Dev'\n",
    "\n",
    "- stage: DeployProd\n",
    "  displayName: 'Deploy to Production'\n",
    "  dependsOn: DeployDev\n",
    "  condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/main'))\n",
    "  jobs:\n",
    "  - deployment: DeployToProd\n",
    "    displayName: 'Deploy to Prod Workspace'\n",
    "    environment: 'fabric-prod'\n",
    "    pool:\n",
    "      vmImage: 'ubuntu-latest'\n",
    "    strategy:\n",
    "      runOnce:\n",
    "        deploy:\n",
    "          steps:\n",
    "          - checkout: self\n",
    "          \n",
    "          - task: AzureCLI@2\n",
    "            inputs:\n",
    "              azureSubscription: 'fabric-service-connection'\n",
    "              scriptType: 'bash'\n",
    "              scriptLocation: 'inlineScript'\n",
    "              inlineScript: |\n",
    "                echo \"Deploying to Production workspace...\"\n",
    "                python scripts/deploy_to_fabric.py --environment prod\n",
    "            displayName: 'Deploy to Production'\n",
    "          \n",
    "          - script: |\n",
    "              python scripts/run_smoke_tests.py\n",
    "            displayName: 'Run Smoke Tests'\n",
    "\"\"\"\n",
    "\n",
    "print(\"Azure DevOps Pipeline YAML:\")\n",
    "print(azure_devops_pipeline)\n",
    "\n",
    "# Save to file\n",
    "with open('/tmp/cicd_config/azure-pipelines.yml', 'w') as f:\n",
    "    f.write(azure_devops_pipeline)\n",
    "print(\"\\n✓ Pipeline saved to /tmp/cicd_config/azure-pipelines.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. GitHub Actions Workflow\n",
    "\n",
    "Alternative CI/CD using GitHub Actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHub Actions workflow\n",
    "github_actions_workflow = \"\"\"\n",
    "# .github/workflows/fabric-cicd.yml\n",
    "name: Fabric CI/CD\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches:\n",
    "      - main\n",
    "      - develop\n",
    "    paths:\n",
    "      - 'Notebooks/**'\n",
    "      - 'DataPipelines/**'\n",
    "      - 'SemanticModels/**'\n",
    "  pull_request:\n",
    "    branches:\n",
    "      - main\n",
    "\n",
    "env:\n",
    "  PYTHON_VERSION: '3.10'\n",
    "\n",
    "jobs:\n",
    "  validate:\n",
    "    name: Validate Fabric Items\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - name: Checkout code\n",
    "        uses: actions/checkout@v3\n",
    "      \n",
    "      - name: Setup Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: ${{ env.PYTHON_VERSION }}\n",
    "      \n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          pip install pytest nbformat nbconvert semantic-link-labs\n",
    "      \n",
    "      - name: Validate notebooks\n",
    "        run: |\n",
    "          python scripts/validate_notebooks.py\n",
    "      \n",
    "      - name: Validate pipelines\n",
    "        run: |\n",
    "          python scripts/validate_pipelines.py\n",
    "      \n",
    "      - name: Run unit tests\n",
    "        run: |\n",
    "          pytest tests/ --junitxml=test-results.xml\n",
    "      \n",
    "      - name: Publish test results\n",
    "        uses: EnricoMi/publish-unit-test-result-action@v2\n",
    "        if: always()\n",
    "        with:\n",
    "          files: test-results.xml\n",
    "\n",
    "  deploy-dev:\n",
    "    name: Deploy to Dev\n",
    "    needs: validate\n",
    "    if: github.ref == 'refs/heads/develop'\n",
    "    runs-on: ubuntu-latest\n",
    "    environment:\n",
    "      name: development\n",
    "    steps:\n",
    "      - name: Checkout code\n",
    "        uses: actions/checkout@v3\n",
    "      \n",
    "      - name: Setup Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: ${{ env.PYTHON_VERSION }}\n",
    "      \n",
    "      - name: Azure Login\n",
    "        uses: azure/login@v1\n",
    "        with:\n",
    "          creds: ${{ secrets.AZURE_CREDENTIALS }}\n",
    "      \n",
    "      - name: Deploy to Dev workspace\n",
    "        run: |\n",
    "          python scripts/deploy_to_fabric.py --environment dev\n",
    "        env:\n",
    "          WORKSPACE_ID: ${{ secrets.DEV_WORKSPACE_ID }}\n",
    "          CLIENT_ID: ${{ secrets.DEV_CLIENT_ID }}\n",
    "          CLIENT_SECRET: ${{ secrets.DEV_CLIENT_SECRET }}\n",
    "          TENANT_ID: ${{ secrets.TENANT_ID }}\n",
    "\n",
    "  deploy-prod:\n",
    "    name: Deploy to Production\n",
    "    needs: validate\n",
    "    if: github.ref == 'refs/heads/main'\n",
    "    runs-on: ubuntu-latest\n",
    "    environment:\n",
    "      name: production\n",
    "      url: https://app.fabric.microsoft.com\n",
    "    steps:\n",
    "      - name: Checkout code\n",
    "        uses: actions/checkout@v3\n",
    "      \n",
    "      - name: Setup Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: ${{ env.PYTHON_VERSION }}\n",
    "      \n",
    "      - name: Azure Login\n",
    "        uses: azure/login@v1\n",
    "        with:\n",
    "          creds: ${{ secrets.AZURE_CREDENTIALS }}\n",
    "      \n",
    "      - name: Deploy to Production workspace\n",
    "        run: |\n",
    "          python scripts/deploy_to_fabric.py --environment prod\n",
    "        env:\n",
    "          WORKSPACE_ID: ${{ secrets.PROD_WORKSPACE_ID }}\n",
    "          CLIENT_ID: ${{ secrets.PROD_CLIENT_ID }}\n",
    "          CLIENT_SECRET: ${{ secrets.PROD_CLIENT_SECRET }}\n",
    "          TENANT_ID: ${{ secrets.TENANT_ID }}\n",
    "      \n",
    "      - name: Run smoke tests\n",
    "        run: |\n",
    "          python scripts/run_smoke_tests.py\n",
    "\"\"\"\n",
    "\n",
    "print(\"GitHub Actions Workflow:\")\n",
    "print(github_actions_workflow)\n",
    "\n",
    "# Save to file\n",
    "with open('/tmp/cicd_config/fabric-cicd.yml', 'w') as f:\n",
    "    f.write(github_actions_workflow)\n",
    "print(\"\\n✓ Workflow saved to /tmp/cicd_config/fabric-cicd.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Validation Scripts\n",
    "\n",
    "Scripts to validate Fabric items before deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook validation script\n",
    "notebook_validator = '''\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"Validate Fabric notebooks\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import nbformat\n",
    "from pathlib import Path\n",
    "\n",
    "def validate_notebook(notebook_path):\n",
    "    \"\"\"Validate a single notebook\"\"\"\n",
    "    errors = []\n",
    "    \n",
    "    try:\n",
    "        # Read notebook\n",
    "        with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "            nb = nbformat.read(f, as_version=4)\n",
    "        \n",
    "        # Check 1: Has cells\n",
    "        if len(nb.cells) == 0:\n",
    "            errors.append(\"Notebook has no cells\")\n",
    "        \n",
    "        # Check 2: Has markdown documentation\n",
    "        markdown_cells = [c for c in nb.cells if c.cell_type == 'markdown']\n",
    "        if len(markdown_cells) == 0:\n",
    "            errors.append(\"No documentation found\")\n",
    "        \n",
    "        # Check 3: No output in cells (clean notebook)\n",
    "        for i, cell in enumerate(nb.cells):\n",
    "            if cell.cell_type == 'code' and cell.get('outputs'):\n",
    "                errors.append(f\"Cell {i} has output (should be cleared)\")\n",
    "        \n",
    "        # Check 4: No hardcoded credentials\n",
    "        dangerous_strings = ['password', 'secret', 'key=', 'token=']\n",
    "        for i, cell in enumerate(nb.cells):\n",
    "            if cell.cell_type == 'code':\n",
    "                source = cell.source.lower()\n",
    "                for danger in dangerous_strings:\n",
    "                    if danger in source and '#' not in source[:source.index(danger)]:\n",
    "                        errors.append(f\"Cell {i} may contain credentials\")\n",
    "        \n",
    "        return len(errors) == 0, errors\n",
    "    \n",
    "    except Exception as e:\n",
    "        return False, [f\"Error reading notebook: {str(e)}\"]\n",
    "\n",
    "def main():\n",
    "    \"\"\"Validate all notebooks\"\"\"\n",
    "    notebook_dir = Path(\"Notebooks\")\n",
    "    \n",
    "    if not notebook_dir.exists():\n",
    "        print(\"No Notebooks directory found\")\n",
    "        return 0\n",
    "    \n",
    "    all_valid = True\n",
    "    notebooks = list(notebook_dir.rglob(\"*.ipynb\"))\n",
    "    \n",
    "    print(f\"Validating {len(notebooks)} notebooks...\\\\n\")\n",
    "    \n",
    "    for nb_path in notebooks:\n",
    "        valid, errors = validate_notebook(nb_path)\n",
    "        \n",
    "        if valid:\n",
    "            print(f\"✓ {nb_path.name}\")\n",
    "        else:\n",
    "            print(f\"✗ {nb_path.name}\")\n",
    "            for error in errors:\n",
    "                print(f\"  - {error}\")\n",
    "            all_valid = False\n",
    "    \n",
    "    return 0 if all_valid else 1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.exit(main())\n",
    "'''\n",
    "\n",
    "print(\"Notebook Validation Script:\")\n",
    "print(notebook_validator)\n",
    "\n",
    "# Save to file\n",
    "with open('/tmp/cicd_config/validate_notebooks.py', 'w') as f:\n",
    "    f.write(notebook_validator)\n",
    "print(\"\\n✓ Script saved to /tmp/cicd_config/validate_notebooks.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline validation script\n",
    "pipeline_validator = '''\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"Validate Fabric data pipelines\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def validate_pipeline(pipeline_path):\n",
    "    \"\"\"Validate a single pipeline definition\"\"\"\n",
    "    errors = []\n",
    "    \n",
    "    try:\n",
    "        with open(pipeline_path, 'r', encoding='utf-8') as f:\n",
    "            pipeline = json.load(f)\n",
    "        \n",
    "        # Check 1: Has activities\n",
    "        if 'properties' not in pipeline or 'activities' not in pipeline['properties']:\n",
    "            errors.append(\"Pipeline has no activities\")\n",
    "        else:\n",
    "            activities = pipeline['properties']['activities']\n",
    "            \n",
    "            # Check 2: All activities have names\n",
    "            for i, activity in enumerate(activities):\n",
    "                if 'name' not in activity:\n",
    "                    errors.append(f\"Activity {i} has no name\")\n",
    "                if 'type' not in activity:\n",
    "                    errors.append(f\"Activity {i} has no type\")\n",
    "        \n",
    "        # Check 3: No hardcoded connections\n",
    "        pipeline_str = json.dumps(pipeline)\n",
    "        if 'AccountKey' in pipeline_str or 'Password' in pipeline_str:\n",
    "            errors.append(\"Pipeline may contain hardcoded credentials\")\n",
    "        \n",
    "        return len(errors) == 0, errors\n",
    "    \n",
    "    except Exception as e:\n",
    "        return False, [f\"Error reading pipeline: {str(e)}\"]\n",
    "\n",
    "def main():\n",
    "    \"\"\"Validate all pipelines\"\"\"\n",
    "    pipeline_dir = Path(\"DataPipelines\")\n",
    "    \n",
    "    if not pipeline_dir.exists():\n",
    "        print(\"No DataPipelines directory found\")\n",
    "        return 0\n",
    "    \n",
    "    all_valid = True\n",
    "    pipelines = list(pipeline_dir.rglob(\"pipeline-content.json\"))\n",
    "    \n",
    "    print(f\"Validating {len(pipelines)} pipelines...\\\\n\")\n",
    "    \n",
    "    for pipeline_path in pipelines:\n",
    "        valid, errors = validate_pipeline(pipeline_path)\n",
    "        \n",
    "        if valid:\n",
    "            print(f\"✓ {pipeline_path.parent.name}\")\n",
    "        else:\n",
    "            print(f\"✗ {pipeline_path.parent.name}\")\n",
    "            for error in errors:\n",
    "                print(f\"  - {error}\")\n",
    "            all_valid = False\n",
    "    \n",
    "    return 0 if all_valid else 1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.exit(main())\n",
    "'''\n",
    "\n",
    "print(\"Pipeline Validation Script:\")\n",
    "print(pipeline_validator)\n",
    "\n",
    "# Save to file\n",
    "with open('/tmp/cicd_config/validate_pipelines.py', 'w') as f:\n",
    "    f.write(pipeline_validator)\n",
    "print(\"\\n✓ Script saved to /tmp/cicd_config/validate_pipelines.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Deployment Script\n",
    "\n",
    "Script to deploy Fabric items using REST API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deployment script\n",
    "deployment_script = '''\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"Deploy Fabric items to workspace\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from azure.identity import DefaultAzureCredential, ClientSecretCredential\n",
    "\n",
    "class FabricDeployer:\n",
    "    \"\"\"Deploy Fabric items using REST API\"\"\"\n",
    "    \n",
    "    def __init__(self, workspace_id, credential):\n",
    "        self.workspace_id = workspace_id\n",
    "        self.credential = credential\n",
    "        self.base_url = \"https://api.fabric.microsoft.com/v1\"\n",
    "        self.token = None\n",
    "    \n",
    "    def get_token(self):\n",
    "        \"\"\"Get access token\"\"\"\n",
    "        if not self.token:\n",
    "            self.token = self.credential.get_token(\n",
    "                \"https://api.fabric.microsoft.com/.default\"\n",
    "            ).token\n",
    "        return self.token\n",
    "    \n",
    "    def deploy_notebook(self, notebook_path, notebook_name):\n",
    "        \"\"\"Deploy a notebook\"\"\"\n",
    "        print(f\"Deploying notebook: {notebook_name}\")\n",
    "        \n",
    "        with open(notebook_path, 'r') as f:\n",
    "            notebook_content = f.read()\n",
    "        \n",
    "        url = f\"{self.base_url}/workspaces/{self.workspace_id}/notebooks\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.get_token()}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        payload = {\n",
    "            \"displayName\": notebook_name,\n",
    "            \"definition\": {\n",
    "                \"parts\": [{\n",
    "                    \"path\": \"notebook-content.py\",\n",
    "                    \"payload\": notebook_content,\n",
    "                    \"payloadType\": \"InlineBase64\"\n",
    "                }]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        \n",
    "        if response.status_code in [200, 201]:\n",
    "            print(f\"  ✓ Successfully deployed {notebook_name}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"  ✗ Failed to deploy {notebook_name}: {response.text}\")\n",
    "            return False\n",
    "    \n",
    "    def deploy_pipeline(self, pipeline_path, pipeline_name):\n",
    "        \"\"\"Deploy a data pipeline\"\"\"\n",
    "        print(f\"Deploying pipeline: {pipeline_name}\")\n",
    "        \n",
    "        with open(pipeline_path, 'r') as f:\n",
    "            pipeline_content = json.load(f)\n",
    "        \n",
    "        url = f\"{self.base_url}/workspaces/{self.workspace_id}/dataPipelines\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.get_token()}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        payload = {\n",
    "            \"displayName\": pipeline_name,\n",
    "            \"definition\": pipeline_content\n",
    "        }\n",
    "        \n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        \n",
    "        if response.status_code in [200, 201]:\n",
    "            print(f\"  ✓ Successfully deployed {pipeline_name}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"  ✗ Failed to deploy {pipeline_name}: {response.text}\")\n",
    "            return False\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Deploy Fabric items')\n",
    "    parser.add_argument('--environment', required=True, choices=['dev', 'test', 'prod'])\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Load environment config\n",
    "    with open('config/environments.json', 'r') as f:\n",
    "        envs = json.load(f)\n",
    "    \n",
    "    env_config = envs.get(args.environment)\n",
    "    if not env_config:\n",
    "        print(f\"Environment {args.environment} not found\")\n",
    "        return 1\n",
    "    \n",
    "    # Create credential\n",
    "    if os.getenv('CLIENT_ID') and os.getenv('CLIENT_SECRET'):\n",
    "        credential = ClientSecretCredential(\n",
    "            tenant_id=env_config['tenant_id'],\n",
    "            client_id=os.getenv('CLIENT_ID'),\n",
    "            client_secret=os.getenv('CLIENT_SECRET')\n",
    "        )\n",
    "    else:\n",
    "        credential = DefaultAzureCredential()\n",
    "    \n",
    "    deployer = FabricDeployer(env_config['workspace_id'], credential)\n",
    "    \n",
    "    print(f\"\\\\nDeploying to {args.environment} environment...\\\\n\")\n",
    "    \n",
    "    success = True\n",
    "    \n",
    "    # Deploy notebooks\n",
    "    notebook_dir = Path(\"Notebooks\")\n",
    "    if notebook_dir.exists():\n",
    "        for nb_path in notebook_dir.rglob(\"*.ipynb\"):\n",
    "            if not deployer.deploy_notebook(nb_path, nb_path.stem):\n",
    "                success = False\n",
    "    \n",
    "    # Deploy pipelines\n",
    "    pipeline_dir = Path(\"DataPipelines\")\n",
    "    if pipeline_dir.exists():\n",
    "        for pipeline_path in pipeline_dir.rglob(\"pipeline-content.json\"):\n",
    "            pipeline_name = pipeline_path.parent.name.replace('.DataPipeline', '')\n",
    "            if not deployer.deploy_pipeline(pipeline_path, pipeline_name):\n",
    "                success = False\n",
    "    \n",
    "    return 0 if success else 1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.exit(main())\n",
    "'''\n",
    "\n",
    "print(\"Deployment Script:\")\n",
    "print(deployment_script)\n",
    "\n",
    "# Save to file\n",
    "with open('/tmp/cicd_config/deploy_to_fabric.py', 'w') as f:\n",
    "    f.write(deployment_script)\n",
    "print(\"\\n✓ Script saved to /tmp/cicd_config/deploy_to_fabric.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Testing Strategy\n",
    "\n",
    "Implement different types of tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test example for data transformation\n",
    "unit_test_example = '''\n",
    "# tests/test_transformations.py\n",
    "import pytest\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Import your transformation functions\n",
    "# from notebooks.transformations import clean_data, aggregate_sales\n",
    "\n",
    "@pytest.fixture\n",
    "def spark():\n",
    "    \"\"\"Create Spark session for testing\"\"\"\n",
    "    return SparkSession.builder \\\\\n",
    "        .appName(\"unit-tests\") \\\\\n",
    "        .master(\"local[*]\") \\\\\n",
    "        .getOrCreate()\n",
    "\n",
    "@pytest.fixture\n",
    "def sample_data():\n",
    "    \"\"\"Create sample test data\"\"\"\n",
    "    return pd.DataFrame({\n",
    "        'id': [1, 2, 3, 4, 5],\n",
    "        'value': [100, 200, None, 400, 500],\n",
    "        'category': ['A', 'B', 'A', 'B', 'C']\n",
    "    })\n",
    "\n",
    "def test_data_cleaning(sample_data):\n",
    "    \"\"\"Test data cleaning removes null values\"\"\"\n",
    "    # cleaned = clean_data(sample_data)\n",
    "    # assert cleaned['value'].isnull().sum() == 0\n",
    "    assert True  # Placeholder\n",
    "\n",
    "def test_aggregation(sample_data):\n",
    "    \"\"\"Test aggregation produces correct results\"\"\"\n",
    "    result = sample_data.groupby('category')['value'].sum()\n",
    "    assert result['A'] == 100  # First value (null excluded)\n",
    "    assert result['B'] == 600  # 200 + 400\n",
    "\n",
    "def test_spark_transformation(spark, sample_data):\n",
    "    \"\"\"Test Spark transformation\"\"\"\n",
    "    df = spark.createDataFrame(sample_data)\n",
    "    assert df.count() == 5\n",
    "    assert len(df.columns) == 3\n",
    "'''\n",
    "\n",
    "print(\"Unit Test Example:\")\n",
    "print(unit_test_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Best Practices\n",
    "\n",
    "### Version Control:\n",
    "1. **Commit frequently** with meaningful messages\n",
    "2. **Use branches** for features and fixes\n",
    "3. **Code review** through pull requests\n",
    "4. **Tag releases** for production deployments\n",
    "\n",
    "### Testing:\n",
    "1. **Unit tests** for transformation logic\n",
    "2. **Integration tests** for pipelines\n",
    "3. **Smoke tests** after deployment\n",
    "4. **Data quality tests** in pipelines\n",
    "\n",
    "### Security:\n",
    "1. **Use Key Vault** for secrets\n",
    "2. **Service Principals** for authentication\n",
    "3. **Never commit credentials** to Git\n",
    "4. **Implement RBAC** for workspaces\n",
    "\n",
    "### Deployment:\n",
    "1. **Automated deployments** for consistency\n",
    "2. **Environment parity** - keep configs similar\n",
    "3. **Rollback plan** for failures\n",
    "4. **Gradual rollout** for major changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Monitoring and Alerting\n",
    "\n",
    "Monitor your CI/CD pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitoring configuration\n",
    "monitoring_config = {\n",
    "    \"metrics_to_track\": [\n",
    "        \"Deployment success rate\",\n",
    "        \"Deployment duration\",\n",
    "        \"Test pass rate\",\n",
    "        \"Pipeline run frequency\",\n",
    "        \"Failed deployment count\"\n",
    "    ],\n",
    "    \"alerts\": [\n",
    "        {\n",
    "            \"name\": \"Deployment Failure\",\n",
    "            \"condition\": \"deployment_status == 'failed'\",\n",
    "            \"action\": \"Send email to data-team@company.com\",\n",
    "            \"severity\": \"High\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Test Failure\",\n",
    "            \"condition\": \"test_pass_rate < 95%\",\n",
    "            \"action\": \"Send Teams notification\",\n",
    "            \"severity\": \"Medium\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Long Deployment\",\n",
    "            \"condition\": \"deployment_duration > 30 minutes\",\n",
    "            \"action\": \"Create incident ticket\",\n",
    "            \"severity\": \"Low\"\n",
    "        }\n",
    "    ],\n",
    "    \"dashboards\": [\n",
    "        {\n",
    "            \"name\": \"CI/CD Overview\",\n",
    "            \"tiles\": [\n",
    "                \"Recent deployments\",\n",
    "                \"Success rate trend\",\n",
    "                \"Average deployment time\",\n",
    "                \"Active branches\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Monitoring Configuration:\")\n",
    "print(json.dumps(monitoring_config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "- ✅ Git integration with Fabric workspaces\n",
    "- ✅ Environment configuration management\n",
    "- ✅ Azure DevOps pipeline setup\n",
    "- ✅ GitHub Actions workflow configuration\n",
    "- ✅ Validation scripts for notebooks and pipelines\n",
    "- ✅ Deployment automation using REST API\n",
    "- ✅ Testing strategies and examples\n",
    "- ✅ Best practices for CI/CD\n",
    "- ✅ Monitoring and alerting setup\n",
    "\n",
    "## Next Steps\n",
    "- Set up your Git repository\n",
    "- Configure service principals\n",
    "- Implement automated tests\n",
    "- Create deployment pipelines\n",
    "- Monitor and iterate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
