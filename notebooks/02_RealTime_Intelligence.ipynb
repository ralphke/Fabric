{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Time Intelligence in Microsoft Fabric\n",
    "\n",
    "This notebook demonstrates the Real-Time Intelligence capabilities in Microsoft Fabric, including:\n",
    "- Event streams\n",
    "- Real-time analytics with KQL\n",
    "- Streaming data ingestion\n",
    "- Real-time dashboards\n",
    "\n",
    "## What is Real-Time Intelligence?\n",
    "\n",
    "Real-Time Intelligence in Fabric provides:\n",
    "- **Event Streams**: Capture, transform, and route streaming data\n",
    "- **KQL Database**: Fast analytics on streaming and historical data\n",
    "- **Real-Time Dashboards**: Visualize streaming data with minimal latency\n",
    "- **Activator**: Trigger actions based on data patterns and conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "To run this notebook, you need:\n",
    "- A Microsoft Fabric workspace with Real-Time Intelligence enabled\n",
    "- A KQL Database (optional)\n",
    "- An Event Stream (optional)\n",
    "- Appropriate permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simulating Streaming Data\n",
    "\n",
    "Let's create a simulator that generates real-time IoT sensor data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Define sensor locations\n",
    "sensor_locations = [\n",
    "    {\"id\": \"sensor_001\", \"location\": \"Building A - Floor 1\", \"type\": \"temperature\"},\n",
    "    {\"id\": \"sensor_002\", \"location\": \"Building A - Floor 2\", \"type\": \"temperature\"},\n",
    "    {\"id\": \"sensor_003\", \"location\": \"Building B - Floor 1\", \"type\": \"humidity\"},\n",
    "    {\"id\": \"sensor_004\", \"location\": \"Building B - Floor 2\", \"type\": \"humidity\"},\n",
    "    {\"id\": \"sensor_005\", \"location\": \"Warehouse\", \"type\": \"pressure\"},\n",
    "]\n",
    "\n",
    "def generate_sensor_reading(sensor):\n",
    "    \"\"\"Generate a single sensor reading\"\"\"\n",
    "    base_values = {\n",
    "        \"temperature\": 22.0,\n",
    "        \"humidity\": 45.0,\n",
    "        \"pressure\": 1013.25\n",
    "    }\n",
    "    \n",
    "    variation = random.uniform(-5, 5)\n",
    "    value = base_values.get(sensor[\"type\"], 0) + variation\n",
    "    \n",
    "    return {\n",
    "        \"sensor_id\": sensor[\"id\"],\n",
    "        \"location\": sensor[\"location\"],\n",
    "        \"type\": sensor[\"type\"],\n",
    "        \"value\": round(value, 2),\n",
    "        \"timestamp\": datetime.utcnow().isoformat(),\n",
    "        \"status\": \"normal\" if abs(variation) < 4 else \"warning\"\n",
    "    }\n",
    "\n",
    "# Generate sample batch of readings\n",
    "sample_readings = [generate_sensor_reading(sensor) for sensor in sensor_locations for _ in range(5)]\n",
    "df_readings = pd.DataFrame(sample_readings)\n",
    "print(\"Sample Sensor Readings:\")\n",
    "display(df_readings.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Working with Event Streams\n",
    "\n",
    "Event Streams in Fabric allow you to:\n",
    "- Ingest data from multiple sources (IoT Hub, Event Hub, Kafka, etc.)\n",
    "- Transform data in real-time\n",
    "- Route data to multiple destinations\n",
    "\n",
    "### Creating an Event Stream (via Fabric UI):\n",
    "1. Navigate to your workspace\n",
    "2. Click \"New\" → \"Event Stream\"\n",
    "3. Configure source connection\n",
    "4. Add transformations if needed\n",
    "5. Configure destination (KQL Database, Lakehouse, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Publishing data to Event Stream using Azure Event Hub SDK\n",
    "# Note: This requires azure-eventhub package and proper credentials\n",
    "\n",
    "# from azure.eventhub import EventHubProducerClient, EventData\n",
    "# \n",
    "# connection_string = \"YOUR_EVENT_HUB_CONNECTION_STRING\"\n",
    "# event_hub_name = \"YOUR_EVENT_HUB_NAME\"\n",
    "# \n",
    "# producer = EventHubProducerClient.from_connection_string(\n",
    "#     connection_string, \n",
    "#     eventhub_name=event_hub_name\n",
    "# )\n",
    "# \n",
    "# # Send batch of events\n",
    "# event_data_batch = producer.create_batch()\n",
    "# for reading in sample_readings:\n",
    "#     event_data_batch.add(EventData(json.dumps(reading)))\n",
    "# \n",
    "# producer.send_batch(event_data_batch)\n",
    "# producer.close()\n",
    "\n",
    "print(\"Event Stream connections are typically configured through the Fabric portal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. KQL (Kusto Query Language) Basics\n",
    "\n",
    "KQL is the query language for Real-Time Intelligence. Let's explore key concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our sample data to demonstrate KQL-like operations in Python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create Spark DataFrame\n",
    "df_spark = spark.createDataFrame(df_readings)\n",
    "\n",
    "# Display schema\n",
    "print(\"Data Schema:\")\n",
    "df_spark.printSchema()\n",
    "\n",
    "# Basic filtering (equivalent to KQL 'where')\n",
    "print(\"\\nWarning readings only:\")\n",
    "df_warnings = df_spark.filter(col(\"status\") == \"warning\")\n",
    "display(df_warnings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Common KQL Query Patterns\n",
    "\n",
    "Here are common query patterns you'll use in KQL Database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KQL Query Examples (to be run in KQL Database)\n",
    "kql_examples = \"\"\"\n",
    "-- Example 1: Basic filtering and projection\n",
    "SensorReadings\n",
    "| where timestamp > ago(1h)\n",
    "| where status == 'warning'\n",
    "| project sensor_id, location, value, timestamp\n",
    "\n",
    "-- Example 2: Aggregation over time windows\n",
    "SensorReadings\n",
    "| where timestamp > ago(24h)\n",
    "| summarize \n",
    "    avg_value = avg(value),\n",
    "    max_value = max(value),\n",
    "    min_value = min(value),\n",
    "    reading_count = count()\n",
    "  by sensor_id, bin(timestamp, 1h)\n",
    "| order by timestamp desc\n",
    "\n",
    "-- Example 3: Anomaly detection\n",
    "SensorReadings\n",
    "| where timestamp > ago(7d)\n",
    "| make-series avg_value = avg(value) on timestamp step 1h by sensor_id\n",
    "| extend anomalies = series_decompose_anomalies(avg_value, 1.5)\n",
    "\n",
    "-- Example 4: Windowed aggregation\n",
    "SensorReadings\n",
    "| where timestamp > ago(1d)\n",
    "| partition by sensor_id\n",
    "  (\n",
    "    order by timestamp asc\n",
    "    | extend moving_avg = row_avg(value, prev(value), prev(value, 2))\n",
    "  )\n",
    "\n",
    "-- Example 5: Join with dimension table\n",
    "SensorReadings\n",
    "| where timestamp > ago(1h)\n",
    "| join kind=inner SensorMetadata on $left.sensor_id == $right.sensor_id\n",
    "| project sensor_id, location, type, value, threshold = metadata_threshold\n",
    "| where value > threshold\n",
    "\"\"\"\n",
    "\n",
    "print(\"Common KQL Query Patterns:\")\n",
    "print(kql_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Real-Time Analytics Scenarios\n",
    "\n",
    "Let's implement some common real-time analytics patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 1: Alert on threshold violations\n",
    "def check_thresholds(df):\n",
    "    \"\"\"Check for threshold violations\"\"\"\n",
    "    thresholds = {\n",
    "        \"temperature\": {\"min\": 18, \"max\": 26},\n",
    "        \"humidity\": {\"min\": 30, \"max\": 60},\n",
    "        \"pressure\": {\"min\": 1000, \"max\": 1025}\n",
    "    }\n",
    "    \n",
    "    alerts = []\n",
    "    for _, row in df.iterrows():\n",
    "        threshold = thresholds.get(row['type'], {})\n",
    "        if threshold:\n",
    "            if row['value'] < threshold['min'] or row['value'] > threshold['max']:\n",
    "                alerts.append({\n",
    "                    'sensor_id': row['sensor_id'],\n",
    "                    'location': row['location'],\n",
    "                    'type': row['type'],\n",
    "                    'value': row['value'],\n",
    "                    'threshold_min': threshold['min'],\n",
    "                    'threshold_max': threshold['max'],\n",
    "                    'timestamp': row['timestamp']\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(alerts)\n",
    "\n",
    "alerts_df = check_thresholds(df_readings)\n",
    "if not alerts_df.empty:\n",
    "    print(f\"Found {len(alerts_df)} threshold violations:\")\n",
    "    display(alerts_df)\n",
    "else:\n",
    "    print(\"No threshold violations detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 2: Calculate moving averages\n",
    "def calculate_moving_average(df, window_size=3):\n",
    "    \"\"\"Calculate moving average for sensor readings\"\"\"\n",
    "    result = df.sort_values('timestamp').copy()\n",
    "    result['moving_avg'] = result.groupby('sensor_id')['value'].transform(\n",
    "        lambda x: x.rolling(window=window_size, min_periods=1).mean()\n",
    "    )\n",
    "    return result\n",
    "\n",
    "df_with_ma = calculate_moving_average(df_readings, window_size=3)\n",
    "print(\"\\nReadings with Moving Average:\")\n",
    "display(df_with_ma[['sensor_id', 'location', 'value', 'moving_avg', 'timestamp']].head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 3: Aggregate by location and time window\n",
    "df_readings_copy = df_readings.copy()\n",
    "df_readings_copy['timestamp'] = pd.to_datetime(df_readings_copy['timestamp'])\n",
    "df_readings_copy['time_bucket'] = df_readings_copy['timestamp'].dt.floor('5min')\n",
    "\n",
    "aggregated = df_readings_copy.groupby(['location', 'type', 'time_bucket']).agg({\n",
    "    'value': ['mean', 'min', 'max', 'std'],\n",
    "    'sensor_id': 'count'\n",
    "}).round(2)\n",
    "\n",
    "aggregated.columns = ['avg_value', 'min_value', 'max_value', 'std_value', 'reading_count']\n",
    "print(\"\\nAggregated readings by location and time:\")\n",
    "display(aggregated.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Setting Up Real-Time Dashboards\n",
    "\n",
    "Real-Time Dashboards in Fabric provide:\n",
    "- Live data visualization\n",
    "- Auto-refresh capabilities\n",
    "- KQL-based queries\n",
    "- Multiple visualization types\n",
    "\n",
    "### Creating a Real-Time Dashboard:\n",
    "1. Navigate to your workspace\n",
    "2. Click \"New\" → \"Real-Time Dashboard\"\n",
    "3. Add tiles with KQL queries\n",
    "4. Configure refresh intervals\n",
    "5. Set up parameters for interactivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample KQL queries for dashboard tiles\n",
    "dashboard_queries = \"\"\"\n",
    "-- Tile 1: Current Status Overview\n",
    "SensorReadings\n",
    "| where timestamp > ago(5m)\n",
    "| summarize \n",
    "    latest_value = arg_max(timestamp, value),\n",
    "    latest_status = arg_max(timestamp, status)\n",
    "  by sensor_id, location\n",
    "| project sensor_id, location, latest_value, latest_status\n",
    "\n",
    "-- Tile 2: Time Series Chart\n",
    "SensorReadings\n",
    "| where timestamp > ago(1h)\n",
    "| where type == 'temperature'\n",
    "| summarize avg_temp = avg(value) by bin(timestamp, 5m), location\n",
    "| render timechart\n",
    "\n",
    "-- Tile 3: Alerts Summary\n",
    "SensorReadings\n",
    "| where timestamp > ago(1h)\n",
    "| where status == 'warning'\n",
    "| summarize alert_count = count() by location\n",
    "| render columnchart\n",
    "\n",
    "-- Tile 4: Real-time Metric\n",
    "SensorReadings\n",
    "| where timestamp > ago(1m)\n",
    "| summarize current_avg = avg(value)\n",
    "| project metric = current_avg\n",
    "| render card\n",
    "\"\"\"\n",
    "\n",
    "print(\"Sample Dashboard Queries:\")\n",
    "print(dashboard_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Activator - Event-Driven Actions\n",
    "\n",
    "Activator enables you to trigger actions based on patterns in your data:\n",
    "\n",
    "### Common Use Cases:\n",
    "- Send email/Teams alerts on threshold violations\n",
    "- Trigger Power Automate flows\n",
    "- Call custom webhooks\n",
    "- Start data pipelines\n",
    "\n",
    "### Setting up Activator:\n",
    "1. Create an Activator item in your workspace\n",
    "2. Connect to your event stream or KQL database\n",
    "3. Define trigger conditions\n",
    "4. Configure actions to take\n",
    "5. Test and activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simulating activator logic\n",
    "def check_activator_conditions(reading):\n",
    "    \"\"\"Check if reading meets activator trigger conditions\"\"\"\n",
    "    triggers = []\n",
    "    \n",
    "    # Condition 1: Temperature too high\n",
    "    if reading['type'] == 'temperature' and reading['value'] > 26:\n",
    "        triggers.append({\n",
    "            'condition': 'High Temperature',\n",
    "            'action': 'Send Alert to Facilities Team',\n",
    "            'severity': 'High',\n",
    "            'details': f\"Temperature at {reading['location']} is {reading['value']}°C\"\n",
    "        })\n",
    "    \n",
    "    # Condition 2: Humidity out of range\n",
    "    if reading['type'] == 'humidity' and (reading['value'] < 30 or reading['value'] > 60):\n",
    "        triggers.append({\n",
    "            'condition': 'Humidity Out of Range',\n",
    "            'action': 'Trigger HVAC Adjustment',\n",
    "            'severity': 'Medium',\n",
    "            'details': f\"Humidity at {reading['location']} is {reading['value']}%\"\n",
    "        })\n",
    "    \n",
    "    return triggers\n",
    "\n",
    "# Check all readings for trigger conditions\n",
    "all_triggers = []\n",
    "for _, reading in df_readings.iterrows():\n",
    "    triggers = check_activator_conditions(reading)\n",
    "    all_triggers.extend(triggers)\n",
    "\n",
    "if all_triggers:\n",
    "    print(f\"\\nFound {len(all_triggers)} activator triggers:\")\n",
    "    display(pd.DataFrame(all_triggers))\n",
    "else:\n",
    "    print(\"\\nNo activator triggers detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Practices for Real-Time Intelligence\n",
    "\n",
    "### Data Ingestion:\n",
    "1. **Use batching** for high-volume streams\n",
    "2. **Implement retry logic** for failed ingestions\n",
    "3. **Monitor ingestion latency** and adjust buffer sizes\n",
    "4. **Use partitioning** for scalability\n",
    "\n",
    "### Query Optimization:\n",
    "1. **Filter early** in your KQL queries\n",
    "2. **Use materialized views** for complex aggregations\n",
    "3. **Implement data retention policies**\n",
    "4. **Use appropriate time ranges** in queries\n",
    "\n",
    "### Monitoring:\n",
    "1. **Set up alerts** for ingestion failures\n",
    "2. **Monitor query performance**\n",
    "3. **Track data freshness**\n",
    "4. **Use capacity metrics** for scaling decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Data quality checks for streaming data\n",
    "def validate_streaming_data(df):\n",
    "    \"\"\"Perform data quality checks\"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    # Check for null values\n",
    "    null_counts = df.isnull().sum()\n",
    "    if null_counts.any():\n",
    "        issues.append(f\"Found null values: {null_counts[null_counts > 0].to_dict()}\")\n",
    "    \n",
    "    # Check for duplicate sensor readings\n",
    "    duplicates = df.duplicated(subset=['sensor_id', 'timestamp']).sum()\n",
    "    if duplicates > 0:\n",
    "        issues.append(f\"Found {duplicates} duplicate readings\")\n",
    "    \n",
    "    # Check value ranges\n",
    "    for sensor_type in df['type'].unique():\n",
    "        type_df = df[df['type'] == sensor_type]\n",
    "        if type_df['value'].std() > 10:  # High variance\n",
    "            issues.append(f\"High variance detected for {sensor_type} sensors\")\n",
    "    \n",
    "    if issues:\n",
    "        print(\"Data Quality Issues:\")\n",
    "        for issue in issues:\n",
    "            print(f\"  - {issue}\")\n",
    "    else:\n",
    "        print(\"✓ All data quality checks passed\")\n",
    "    \n",
    "    return len(issues) == 0\n",
    "\n",
    "validate_streaming_data(df_readings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Integration with Other Fabric Services\n",
    "\n",
    "Real-Time Intelligence integrates seamlessly with:\n",
    "\n",
    "### OneLake:\n",
    "- Store historical streaming data in Delta tables\n",
    "- Combine real-time and batch analytics\n",
    "\n",
    "### Data Pipelines:\n",
    "- Trigger pipelines from events\n",
    "- Orchestrate data processing workflows\n",
    "\n",
    "### Power BI:\n",
    "- Create real-time reports\n",
    "- Use DirectQuery with KQL databases\n",
    "\n",
    "### Data Science:\n",
    "- Apply ML models to streaming data\n",
    "- Real-time predictions and scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Archiving streaming data to OneLake\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"sensor_id\", StringType(), False),\n",
    "    StructField(\"location\", StringType(), False),\n",
    "    StructField(\"type\", StringType(), False),\n",
    "    StructField(\"value\", DoubleType(), False),\n",
    "    StructField(\"timestamp\", StringType(), False),\n",
    "    StructField(\"status\", StringType(), False)\n",
    "])\n",
    "\n",
    "df_archive = spark.createDataFrame(df_readings.values.tolist(), schema)\n",
    "\n",
    "# Write to OneLake (Delta format)\n",
    "# df_archive.write.format(\"delta\") \\\n",
    "#     .mode(\"append\") \\\n",
    "#     .partitionBy(\"type\") \\\n",
    "#     .save(\"Tables/sensor_archive\")\n",
    "\n",
    "print(\"Streaming data can be archived to OneLake for long-term storage and analysis\")\n",
    "print(f\"Sample data shape: {df_archive.count()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "- ✅ Real-Time Intelligence architecture and components\n",
    "- ✅ Event Streams for data ingestion\n",
    "- ✅ KQL query language and patterns\n",
    "- ✅ Real-time analytics scenarios\n",
    "- ✅ Real-Time Dashboards for visualization\n",
    "- ✅ Activator for event-driven actions\n",
    "- ✅ Best practices for streaming analytics\n",
    "- ✅ Integration with other Fabric services\n",
    "\n",
    "## Next Steps\n",
    "- Explore Power BI semantic models for reporting\n",
    "- Learn about data pipeline orchestration\n",
    "- Implement CI/CD for Fabric solutions\n",
    "- Build end-to-end streaming applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
