{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipelines in Microsoft Fabric\n",
    "\n",
    "This notebook demonstrates building and orchestrating data pipelines in Microsoft Fabric.\n",
    "\n",
    "## What are Data Pipelines?\n",
    "\n",
    "Data Pipelines in Fabric provide:\n",
    "- **Visual orchestration** of data workflows\n",
    "- **150+ connectors** to various data sources\n",
    "- **Data transformation** using Copy, Dataflow, and Notebook activities\n",
    "- **Scheduling and triggers** for automation\n",
    "- **Monitoring and alerts** for reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "To follow this notebook, you need:\n",
    "- A Microsoft Fabric workspace\n",
    "- Source data (can be files, databases, APIs)\n",
    "- Destination (Lakehouse, Warehouse, etc.)\n",
    "- Appropriate permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pipeline Architecture Patterns\n",
    "\n",
    "### Common Pipeline Patterns:\n",
    "1. **ETL (Extract, Transform, Load)**: Traditional data warehousing\n",
    "2. **ELT (Extract, Load, Transform)**: Modern data lake approach\n",
    "3. **Change Data Capture (CDC)**: Incremental updates\n",
    "4. **Lambda Architecture**: Batch + Real-time processing\n",
    "5. **Medallion Architecture**: Bronze, Silver, Gold layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize pipeline patterns\n",
    "pipeline_patterns = {\n",
    "    \"ETL Pattern\": [\n",
    "        \"1. Extract data from sources\",\n",
    "        \"2. Transform in staging area\",\n",
    "        \"3. Load to target system\",\n",
    "        \"Best for: Traditional DW, Small-medium data volumes\"\n",
    "    ],\n",
    "    \"ELT Pattern\": [\n",
    "        \"1. Extract data from sources\",\n",
    "        \"2. Load raw data to data lake\",\n",
    "        \"3. Transform using compute engine\",\n",
    "        \"Best for: Cloud platforms, Large data volumes\"\n",
    "    ],\n",
    "    \"Medallion Architecture\": [\n",
    "        \"Bronze: Raw data ingestion (as-is)\",\n",
    "        \"Silver: Cleaned and conformed data\",\n",
    "        \"Gold: Business-level aggregations\",\n",
    "        \"Best for: Data lakes, Multi-purpose analytics\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for pattern, steps in pipeline_patterns.items():\n",
    "    print(f\"\\n{pattern}:\")\n",
    "    for step in steps:\n",
    "        print(f\"  {step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating Sample Source Data\n",
    "\n",
    "Let's create sample data that we'll use in our pipeline examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "# Generate sample transaction data\n",
    "np.random.seed(42)\n",
    "n_records = 1000\n",
    "\n",
    "transactions = pd.DataFrame({\n",
    "    'transaction_id': [f'TXN{str(i).zfill(6)}' for i in range(1, n_records + 1)],\n",
    "    'customer_id': [f'CUST{np.random.randint(1, 201):04d}' for _ in range(n_records)],\n",
    "    'product_id': [f'PROD{np.random.randint(1, 51):03d}' for _ in range(n_records)],\n",
    "    'transaction_date': [datetime.now() - timedelta(days=np.random.randint(0, 90)) for _ in range(n_records)],\n",
    "    'quantity': np.random.randint(1, 11, n_records),\n",
    "    'unit_price': np.round(np.random.uniform(10, 500, n_records), 2),\n",
    "    'status': np.random.choice(['completed', 'pending', 'cancelled'], n_records, p=[0.85, 0.10, 0.05])\n",
    "})\n",
    "\n",
    "transactions['total_amount'] = (transactions['quantity'] * transactions['unit_price']).round(2)\n",
    "\n",
    "print(f\"Generated {len(transactions)} sample transactions\")\n",
    "print(f\"Date range: {transactions['transaction_date'].min()} to {transactions['transaction_date'].max()}\")\n",
    "print(f\"Total value: ${transactions['total_amount'].sum():,.2f}\")\n",
    "display(transactions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save sample data to files (simulating source data)\n",
    "import os\n",
    "\n",
    "# Create temp directory for sample files\n",
    "os.makedirs('/tmp/pipeline_data', exist_ok=True)\n",
    "\n",
    "# Save as CSV\n",
    "transactions.to_csv('/tmp/pipeline_data/transactions.csv', index=False)\n",
    "print(\"✓ Saved transactions.csv\")\n",
    "\n",
    "# Save as JSON\n",
    "transactions.to_json('/tmp/pipeline_data/transactions.json', orient='records', date_format='iso')\n",
    "print(\"✓ Saved transactions.json\")\n",
    "\n",
    "# Save as Parquet\n",
    "transactions.to_parquet('/tmp/pipeline_data/transactions.parquet', index=False)\n",
    "print(\"✓ Saved transactions.parquet\")\n",
    "\n",
    "print(\"\\nSample files created in /tmp/pipeline_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pipeline Activities\n",
    "\n",
    "### Key Activity Types:\n",
    "1. **Copy Data**: Transfer data between sources and destinations\n",
    "2. **Dataflow**: Transform data using Power Query\n",
    "3. **Notebook**: Execute Python/Scala/R code\n",
    "4. **Stored Procedure**: Run database procedures\n",
    "5. **Script**: Execute custom scripts\n",
    "6. **Control Flow**: If, ForEach, Until, Wait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building a Pipeline (Conceptual)\n",
    "\n",
    "### Pipeline Creation Steps:\n",
    "1. Navigate to Fabric workspace\n",
    "2. Create New → Data Pipeline\n",
    "3. Add activities from the toolbar\n",
    "4. Configure activity properties\n",
    "5. Connect activities (success/failure paths)\n",
    "6. Set up parameters and variables\n",
    "7. Test and debug\n",
    "8. Schedule or trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Pipeline Structure (JSON representation)\n",
    "pipeline_definition = {\n",
    "    \"name\": \"Sales_Data_Pipeline\",\n",
    "    \"properties\": {\n",
    "        \"description\": \"Daily sales data processing pipeline\",\n",
    "        \"activities\": [\n",
    "            {\n",
    "                \"name\": \"Copy_CSV_to_Bronze\",\n",
    "                \"type\": \"Copy\",\n",
    "                \"description\": \"Copy raw data to Bronze layer\",\n",
    "                \"source\": {\n",
    "                    \"type\": \"DelimitedText\",\n",
    "                    \"location\": \"Files/source/transactions.csv\"\n",
    "                },\n",
    "                \"sink\": {\n",
    "                    \"type\": \"DeltaTable\",\n",
    "                    \"location\": \"Tables/Bronze_Transactions\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Transform_Bronze_to_Silver\",\n",
    "                \"type\": \"Notebook\",\n",
    "                \"description\": \"Clean and validate data\",\n",
    "                \"notebookPath\": \"Notebooks/Transform_to_Silver\",\n",
    "                \"dependsOn\": [\"Copy_CSV_to_Bronze\"]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Aggregate_Silver_to_Gold\",\n",
    "                \"type\": \"Notebook\",\n",
    "                \"description\": \"Create business aggregations\",\n",
    "                \"notebookPath\": \"Notebooks/Aggregate_to_Gold\",\n",
    "                \"dependsOn\": [\"Transform_Bronze_to_Silver\"]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Refresh_Semantic_Model\",\n",
    "                \"type\": \"ExecuteDataflow\",\n",
    "                \"description\": \"Refresh Power BI semantic model\",\n",
    "                \"dependsOn\": [\"Aggregate_Silver_to_Gold\"]\n",
    "            }\n",
    "        ],\n",
    "        \"parameters\": {\n",
    "            \"ProcessDate\": {\n",
    "                \"type\": \"String\",\n",
    "                \"defaultValue\": \"@utcnow()\"\n",
    "            },\n",
    "            \"SourcePath\": {\n",
    "                \"type\": \"String\",\n",
    "                \"defaultValue\": \"Files/source/\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Sample Pipeline Definition:\")\n",
    "print(json.dumps(pipeline_definition, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Transformation Examples\n",
    "\n",
    "Let's simulate the transformations that would happen in pipeline activities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bronze Layer: Raw data ingestion (minimal processing)\n",
    "def bronze_layer(df):\n",
    "    \"\"\"Bronze layer: Ingest raw data with metadata\"\"\"\n",
    "    df_bronze = df.copy()\n",
    "    df_bronze['_ingestion_timestamp'] = datetime.now()\n",
    "    df_bronze['_source_file'] = 'transactions.csv'\n",
    "    return df_bronze\n",
    "\n",
    "bronze_transactions = bronze_layer(transactions)\n",
    "print(\"Bronze Layer:\")\n",
    "print(f\"  Records: {len(bronze_transactions)}\")\n",
    "print(f\"  Columns: {list(bronze_transactions.columns)}\")\n",
    "display(bronze_transactions.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silver Layer: Cleaned and validated data\n",
    "def silver_layer(df_bronze):\n",
    "    \"\"\"Silver layer: Clean, validate, and standardize\"\"\"\n",
    "    df_silver = df_bronze.copy()\n",
    "    \n",
    "    # Data quality checks\n",
    "    # 1. Remove cancelled transactions\n",
    "    df_silver = df_silver[df_silver['status'] != 'cancelled']\n",
    "    \n",
    "    # 2. Remove duplicates\n",
    "    df_silver = df_silver.drop_duplicates(subset=['transaction_id'])\n",
    "    \n",
    "    # 3. Handle invalid values\n",
    "    df_silver = df_silver[df_silver['quantity'] > 0]\n",
    "    df_silver = df_silver[df_silver['unit_price'] > 0]\n",
    "    \n",
    "    # 4. Add calculated fields\n",
    "    df_silver['transaction_year'] = df_silver['transaction_date'].dt.year\n",
    "    df_silver['transaction_month'] = df_silver['transaction_date'].dt.month\n",
    "    df_silver['transaction_quarter'] = df_silver['transaction_date'].dt.quarter\n",
    "    \n",
    "    # 5. Add data quality flag\n",
    "    df_silver['data_quality_score'] = 100  # Could be more complex\n",
    "    df_silver['_processing_timestamp'] = datetime.now()\n",
    "    \n",
    "    return df_silver\n",
    "\n",
    "silver_transactions = silver_layer(bronze_transactions)\n",
    "print(\"\\nSilver Layer:\")\n",
    "print(f\"  Records: {len(silver_transactions)} (removed {len(bronze_transactions) - len(silver_transactions)} invalid records)\")\n",
    "print(f\"  Quality Score: {silver_transactions['data_quality_score'].mean():.1f}%\")\n",
    "display(silver_transactions.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gold Layer: Business-level aggregations\n",
    "def gold_layer(df_silver):\n",
    "    \"\"\"Gold layer: Create business aggregations\"\"\"\n",
    "    \n",
    "    # Daily aggregations\n",
    "    gold_daily = df_silver.groupby(\n",
    "        df_silver['transaction_date'].dt.date\n",
    "    ).agg({\n",
    "        'transaction_id': 'count',\n",
    "        'total_amount': ['sum', 'mean'],\n",
    "        'quantity': 'sum',\n",
    "        'customer_id': 'nunique'\n",
    "    }).round(2)\n",
    "    \n",
    "    gold_daily.columns = ['transaction_count', 'total_revenue', 'avg_order_value', 'total_units', 'unique_customers']\n",
    "    gold_daily = gold_daily.reset_index()\n",
    "    gold_daily.columns = ['date', 'transaction_count', 'total_revenue', 'avg_order_value', 'total_units', 'unique_customers']\n",
    "    \n",
    "    # Product aggregations\n",
    "    gold_product = df_silver.groupby('product_id').agg({\n",
    "        'transaction_id': 'count',\n",
    "        'quantity': 'sum',\n",
    "        'total_amount': 'sum'\n",
    "    }).round(2)\n",
    "    \n",
    "    gold_product.columns = ['order_count', 'units_sold', 'revenue']\n",
    "    gold_product = gold_product.sort_values('revenue', ascending=False).reset_index()\n",
    "    \n",
    "    # Customer aggregations\n",
    "    gold_customer = df_silver.groupby('customer_id').agg({\n",
    "        'transaction_id': 'count',\n",
    "        'total_amount': ['sum', 'mean'],\n",
    "        'transaction_date': ['min', 'max']\n",
    "    }).round(2)\n",
    "    \n",
    "    gold_customer.columns = ['purchase_count', 'total_spent', 'avg_order_value', 'first_purchase', 'last_purchase']\n",
    "    gold_customer = gold_customer.reset_index()\n",
    "    \n",
    "    return {\n",
    "        'daily_metrics': gold_daily,\n",
    "        'product_metrics': gold_product,\n",
    "        'customer_metrics': gold_customer\n",
    "    }\n",
    "\n",
    "gold_tables = gold_layer(silver_transactions)\n",
    "\n",
    "print(\"\\nGold Layer - Daily Metrics:\")\n",
    "display(gold_tables['daily_metrics'].tail(10))\n",
    "\n",
    "print(\"\\nGold Layer - Top Products:\")\n",
    "display(gold_tables['product_metrics'].head(10))\n",
    "\n",
    "print(\"\\nGold Layer - Top Customers:\")\n",
    "top_customers = gold_tables['customer_metrics'].sort_values('total_spent', ascending=False).head(10)\n",
    "display(top_customers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Incremental Loading Pattern\n",
    "\n",
    "Incremental loading processes only new or changed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incremental_load(df_source, df_target, key_column, timestamp_column):\n",
    "    \"\"\"\n",
    "    Implement incremental load logic\n",
    "    \n",
    "    Args:\n",
    "        df_source: New data from source\n",
    "        df_target: Existing data in target\n",
    "        key_column: Unique identifier column\n",
    "        timestamp_column: Column to track changes\n",
    "    \"\"\"\n",
    "    if df_target is None or len(df_target) == 0:\n",
    "        # First load - load all data\n",
    "        print(\"Initial load: Loading all records\")\n",
    "        return df_source, len(df_source), 0, 0\n",
    "    \n",
    "    # Get max timestamp from target\n",
    "    max_timestamp = df_target[timestamp_column].max()\n",
    "    \n",
    "    # Get new records\n",
    "    df_new = df_source[df_source[timestamp_column] > max_timestamp]\n",
    "    \n",
    "    # Get updated records (exists in target but newer in source)\n",
    "    existing_keys = set(df_target[key_column])\n",
    "    source_keys = set(df_source[key_column])\n",
    "    \n",
    "    updated_keys = existing_keys.intersection(source_keys)\n",
    "    df_updates = df_source[\n",
    "        (df_source[key_column].isin(updated_keys)) & \n",
    "        (df_source[timestamp_column] > max_timestamp)\n",
    "    ]\n",
    "    \n",
    "    # Combine new and updated records\n",
    "    df_incremental = pd.concat([df_new, df_updates]).drop_duplicates(subset=[key_column])\n",
    "    \n",
    "    print(f\"Incremental load:\")\n",
    "    print(f\"  New records: {len(df_new)}\")\n",
    "    print(f\"  Updated records: {len(df_updates)}\")\n",
    "    print(f\"  Total to process: {len(df_incremental)}\")\n",
    "    \n",
    "    return df_incremental, len(df_new), len(df_updates), len(df_target)\n",
    "\n",
    "# Simulate incremental load\n",
    "# First run - empty target\n",
    "print(\"\\n=== First Pipeline Run ===\")\n",
    "incremental_data, new_count, updated_count, existing_count = incremental_load(\n",
    "    transactions, None, 'transaction_id', 'transaction_date'\n",
    ")\n",
    "\n",
    "# Second run - with new data\n",
    "print(\"\\n=== Second Pipeline Run (with new data) ===\")\n",
    "new_transactions = transactions.sample(n=50).copy()\n",
    "new_transactions['transaction_date'] = datetime.now()\n",
    "\n",
    "incremental_data, new_count, updated_count, existing_count = incremental_load(\n",
    "    new_transactions, transactions, 'transaction_id', 'transaction_date'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Handling and Logging\n",
    "\n",
    "Proper error handling is crucial for production pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('DataPipeline')\n",
    "\n",
    "class PipelineMonitor:\n",
    "    \"\"\"Monitor pipeline execution\"\"\"\n",
    "    \n",
    "    def __init__(self, pipeline_name):\n",
    "        self.pipeline_name = pipeline_name\n",
    "        self.start_time = None\n",
    "        self.metrics = {\n",
    "            'records_processed': 0,\n",
    "            'records_failed': 0,\n",
    "            'errors': []\n",
    "        }\n",
    "    \n",
    "    def start(self):\n",
    "        \"\"\"Start pipeline monitoring\"\"\"\n",
    "        self.start_time = datetime.now()\n",
    "        logger.info(f\"Pipeline '{self.pipeline_name}' started at {self.start_time}\")\n",
    "    \n",
    "    def log_activity(self, activity_name, status, records=0):\n",
    "        \"\"\"Log activity execution\"\"\"\n",
    "        logger.info(f\"Activity '{activity_name}': {status} - {records} records\")\n",
    "        if status == 'success':\n",
    "            self.metrics['records_processed'] += records\n",
    "        else:\n",
    "            self.metrics['records_failed'] += records\n",
    "    \n",
    "    def log_error(self, activity_name, error):\n",
    "        \"\"\"Log error\"\"\"\n",
    "        error_msg = f\"{activity_name}: {str(error)}\"\n",
    "        logger.error(error_msg)\n",
    "        self.metrics['errors'].append(error_msg)\n",
    "    \n",
    "    def complete(self):\n",
    "        \"\"\"Complete pipeline monitoring\"\"\"\n",
    "        duration = (datetime.now() - self.start_time).total_seconds()\n",
    "        logger.info(f\"Pipeline '{self.pipeline_name}' completed in {duration:.2f} seconds\")\n",
    "        logger.info(f\"Metrics: {self.metrics}\")\n",
    "        return self.metrics\n",
    "\n",
    "# Example usage\n",
    "monitor = PipelineMonitor(\"Sales_ETL_Pipeline\")\n",
    "monitor.start()\n",
    "\n",
    "try:\n",
    "    # Simulate activities\n",
    "    monitor.log_activity(\"Copy_to_Bronze\", \"success\", len(transactions))\n",
    "    monitor.log_activity(\"Transform_to_Silver\", \"success\", len(silver_transactions))\n",
    "    monitor.log_activity(\"Aggregate_to_Gold\", \"success\", len(gold_tables['daily_metrics']))\n",
    "except Exception as e:\n",
    "    monitor.log_error(\"Pipeline_Execution\", e)\n",
    "finally:\n",
    "    final_metrics = monitor.complete()\n",
    "    print(\"\\nPipeline Metrics:\")\n",
    "    print(json.dumps(final_metrics, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Pipeline Parameters and Variables\n",
    "\n",
    "Parameters make pipelines flexible and reusable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example pipeline parameters\n",
    "pipeline_parameters = {\n",
    "    # Static parameters (set at pipeline creation)\n",
    "    \"source_container\": \"raw-data\",\n",
    "    \"target_lakehouse\": \"enterprise_lakehouse\",\n",
    "    \"email_notifications\": \"data-team@company.com\",\n",
    "    \n",
    "    # Dynamic parameters (can be passed at runtime)\n",
    "    \"process_date\": \"@formatDateTime(utcnow(), 'yyyy-MM-dd')\",\n",
    "    \"batch_size\": 10000,\n",
    "    \"retry_count\": 3,\n",
    "    \n",
    "    # Environment-specific parameters\n",
    "    \"environment\": \"production\",\n",
    "    \"logging_level\": \"INFO\"\n",
    "}\n",
    "\n",
    "# Pipeline variables (set during execution)\n",
    "pipeline_variables = {\n",
    "    \"records_processed\": 0,\n",
    "    \"last_success_time\": None,\n",
    "    \"retry_attempt\": 0,\n",
    "    \"error_message\": \"\"\n",
    "}\n",
    "\n",
    "print(\"Pipeline Parameters:\")\n",
    "print(json.dumps(pipeline_parameters, indent=2))\n",
    "\n",
    "print(\"\\nPipeline Variables:\")\n",
    "print(json.dumps(pipeline_variables, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Scheduling and Triggers\n",
    "\n",
    "### Trigger Types:\n",
    "1. **Schedule Trigger**: Run on a schedule (cron expression)\n",
    "2. **Event Trigger**: Run when a file arrives or event occurs\n",
    "3. **Manual Trigger**: Run on-demand\n",
    "4. **Tumbling Window**: Fixed-size, non-overlapping time windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example trigger configurations\n",
    "trigger_examples = {\n",
    "    \"daily_schedule\": {\n",
    "        \"type\": \"ScheduleTrigger\",\n",
    "        \"properties\": {\n",
    "            \"recurrence\": {\n",
    "                \"frequency\": \"Day\",\n",
    "                \"interval\": 1,\n",
    "                \"startTime\": \"2024-01-01T02:00:00Z\",\n",
    "                \"timeZone\": \"UTC\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"hourly_schedule\": {\n",
    "        \"type\": \"ScheduleTrigger\",\n",
    "        \"properties\": {\n",
    "            \"recurrence\": {\n",
    "                \"frequency\": \"Hour\",\n",
    "                \"interval\": 1\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"file_arrival\": {\n",
    "        \"type\": \"BlobEventsTrigger\",\n",
    "        \"properties\": {\n",
    "            \"events\": [\"Microsoft.Storage.BlobCreated\"],\n",
    "            \"scope\": \"/subscriptions/{subscription}/resourceGroups/{rg}/providers/Microsoft.Storage/storageAccounts/{account}\",\n",
    "            \"blobPathBeginsWith\": \"/raw-data/transactions/\",\n",
    "            \"blobPathEndsWith\": \".csv\"\n",
    "        }\n",
    "    },\n",
    "    \"tumbling_window\": {\n",
    "        \"type\": \"TumblingWindowTrigger\",\n",
    "        \"properties\": {\n",
    "            \"frequency\": \"Hour\",\n",
    "            \"interval\": 1,\n",
    "            \"startTime\": \"2024-01-01T00:00:00Z\",\n",
    "            \"delay\": \"00:15:00\",  # 15-minute delay\n",
    "            \"maxConcurrency\": 1,\n",
    "            \"retryPolicy\": {\n",
    "                \"count\": 3,\n",
    "                \"intervalInSeconds\": 30\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Trigger Configuration Examples:\")\n",
    "for trigger_name, config in trigger_examples.items():\n",
    "    print(f\"\\n{trigger_name}:\")\n",
    "    print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Best Practices\n",
    "\n",
    "### Design Principles:\n",
    "1. **Idempotency**: Pipelines should produce same result when run multiple times\n",
    "2. **Modularity**: Break complex logic into reusable components\n",
    "3. **Error Handling**: Implement retry logic and failure notifications\n",
    "4. **Logging**: Comprehensive logging for debugging\n",
    "5. **Testing**: Test with sample data before production\n",
    "\n",
    "### Performance:\n",
    "1. **Parallel Processing**: Use ForEach activities with concurrency\n",
    "2. **Partitioning**: Process data in chunks\n",
    "3. **Compression**: Use compressed formats (Parquet, Avro)\n",
    "4. **Incremental Loads**: Process only new/changed data\n",
    "\n",
    "### Monitoring:\n",
    "1. **Pipeline Runs**: Monitor success/failure rates\n",
    "2. **Duration Tracking**: Alert on long-running pipelines\n",
    "3. **Data Quality**: Implement quality checks\n",
    "4. **Alerting**: Configure notifications for failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality checks example\n",
    "def data_quality_checks(df, table_name):\n",
    "    \"\"\"Run data quality validations\"\"\"\n",
    "    checks = []\n",
    "    \n",
    "    # Check 1: No null values in key columns\n",
    "    key_columns = ['transaction_id', 'customer_id', 'product_id']\n",
    "    for col in key_columns:\n",
    "        null_count = df[col].isnull().sum()\n",
    "        checks.append({\n",
    "            'check': f'No nulls in {col}',\n",
    "            'passed': null_count == 0,\n",
    "            'details': f'{null_count} null values found'\n",
    "        })\n",
    "    \n",
    "    # Check 2: Positive values\n",
    "    checks.append({\n",
    "        'check': 'All quantities positive',\n",
    "        'passed': (df['quantity'] > 0).all(),\n",
    "        'details': f\"{(df['quantity'] <= 0).sum()} negative/zero values\"\n",
    "    })\n",
    "    \n",
    "    # Check 3: Reasonable date range\n",
    "    days_old = (datetime.now() - df['transaction_date'].min()).days\n",
    "    checks.append({\n",
    "        'check': 'Date range reasonable',\n",
    "        'passed': days_old <= 365,\n",
    "        'details': f'Oldest record: {days_old} days'\n",
    "    })\n",
    "    \n",
    "    # Check 4: No duplicates\n",
    "    duplicate_count = df.duplicated(subset=['transaction_id']).sum()\n",
    "    checks.append({\n",
    "        'check': 'No duplicate IDs',\n",
    "        'passed': duplicate_count == 0,\n",
    "        'details': f'{duplicate_count} duplicates found'\n",
    "    })\n",
    "    \n",
    "    # Summary\n",
    "    passed = sum(1 for c in checks if c['passed'])\n",
    "    total = len(checks)\n",
    "    \n",
    "    print(f\"\\nData Quality Report - {table_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for check in checks:\n",
    "        status = '✓' if check['passed'] else '✗'\n",
    "        print(f\"{status} {check['check']}: {check['details']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Passed: {passed}/{total} checks ({passed/total*100:.1f}%)\")\n",
    "    \n",
    "    return passed == total\n",
    "\n",
    "# Run quality checks\n",
    "quality_passed = data_quality_checks(transactions, 'Transactions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "- ✅ Pipeline architecture patterns (ETL, ELT, Medallion)\n",
    "- ✅ Key pipeline activities and components\n",
    "- ✅ Building multi-layer data transformations\n",
    "- ✅ Implementing incremental loading\n",
    "- ✅ Error handling and monitoring\n",
    "- ✅ Pipeline parameters and variables\n",
    "- ✅ Scheduling and trigger configuration\n",
    "- ✅ Data quality validation\n",
    "- ✅ Best practices for production pipelines\n",
    "\n",
    "## Next Steps\n",
    "- Implement CI/CD for pipelines\n",
    "- Integrate with DevOps practices\n",
    "- Build real-time streaming pipelines\n",
    "- Explore advanced orchestration patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
