{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OneLake Introduction and Capabilities\n",
    "\n",
    "This notebook demonstrates the key capabilities of Microsoft Fabric OneLake - the unified data lake for the entire organization.\n",
    "\n",
    "## What is OneLake?\n",
    "\n",
    "OneLake is a unified, logical data lake for your entire organization. It provides:\n",
    "- **Single storage location** for all your analytics data\n",
    "- **Automatic integration** with all Fabric workloads\n",
    "- **Delta Parquet format** for efficient storage and querying\n",
    "- **Lakehouse architecture** combining data lake and data warehouse capabilities\n",
    "- **Shortcuts** to external data sources without data movement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "To run this notebook, you need:\n",
    "- A Microsoft Fabric workspace\n",
    "- A Lakehouse created in your workspace\n",
    "- Appropriate permissions to read/write data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connecting to OneLake\n",
    "\n",
    "In Fabric notebooks, you're automatically connected to OneLake. Let's verify the connection and explore the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Display Spark configuration\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Application Name: {spark.sparkContext.appName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reading Data from OneLake\n",
    "\n",
    "OneLake supports multiple file formats. Let's demonstrate reading different formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Reading CSV data from OneLake\n",
    "# Replace with your actual path\n",
    "# csv_path = \"Files/sample_data.csv\"\n",
    "# df_csv = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "# display(df_csv.limit(10))\n",
    "\n",
    "print(\"To read CSV files, use: spark.read.csv(path, header=True, inferSchema=True)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Reading Parquet data from OneLake\n",
    "# parquet_path = \"Files/sample_data.parquet\"\n",
    "# df_parquet = spark.read.parquet(parquet_path)\n",
    "# display(df_parquet.limit(10))\n",
    "\n",
    "print(\"To read Parquet files, use: spark.read.parquet(path)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Reading Delta tables from OneLake\n",
    "# delta_table_path = \"Tables/sales_data\"\n",
    "# df_delta = spark.read.format(\"delta\").load(delta_table_path)\n",
    "# display(df_delta.limit(10))\n",
    "\n",
    "print(\"To read Delta tables, use: spark.read.format('delta').load(path)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating Sample Data\n",
    "\n",
    "Let's create some sample data to demonstrate OneLake capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample sales data\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Generate sample data\n",
    "sample_data = []\n",
    "start_date = datetime(2024, 1, 1)\n",
    "products = ['Laptop', 'Phone', 'Tablet', 'Monitor', 'Keyboard']\n",
    "regions = ['North', 'South', 'East', 'West']\n",
    "\n",
    "for i in range(100):\n",
    "    sample_data.append((\n",
    "        i + 1,\n",
    "        random.choice(products),\n",
    "        random.choice(regions),\n",
    "        random.randint(1, 10),\n",
    "        round(random.uniform(100, 2000), 2),\n",
    "        start_date + timedelta(days=random.randint(0, 365))\n",
    "    ))\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"OrderID\", IntegerType(), False),\n",
    "    StructField(\"Product\", StringType(), False),\n",
    "    StructField(\"Region\", StringType(), False),\n",
    "    StructField(\"Quantity\", IntegerType(), False),\n",
    "    StructField(\"Amount\", DoubleType(), False),\n",
    "    StructField(\"OrderDate\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "df_sales = spark.createDataFrame(sample_data, schema)\n",
    "display(df_sales.limit(10))\n",
    "print(f\"Total records: {df_sales.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Writing Data to OneLake\n",
    "\n",
    "Demonstrate different ways to write data to OneLake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write as Delta table (recommended for OneLake)\n",
    "# This provides ACID transactions, time travel, and optimal performance\n",
    "delta_table_name = \"sales_demo\"\n",
    "\n",
    "# Write to Delta table\n",
    "df_sales.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"Tables/{delta_table_name}\")\n",
    "\n",
    "print(f\"Data written to Delta table: {delta_table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write as Parquet files\n",
    "df_sales.write.mode(\"overwrite\") \\\n",
    "    .parquet(\"Files/sales_parquet\")\n",
    "\n",
    "print(\"Data written to Parquet format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Querying Data with SQL\n",
    "\n",
    "OneLake supports SQL queries through Spark SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register DataFrame as temp view\n",
    "df_sales.createOrReplaceTempView(\"sales\")\n",
    "\n",
    "# Run SQL query\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        Region,\n",
    "        Product,\n",
    "        COUNT(*) as OrderCount,\n",
    "        SUM(Quantity) as TotalQuantity,\n",
    "        ROUND(SUM(Amount), 2) as TotalRevenue\n",
    "    FROM sales\n",
    "    GROUP BY Region, Product\n",
    "    ORDER BY TotalRevenue DESC\n",
    "\"\"\")\n",
    "\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Transformations\n",
    "\n",
    "Demonstrate common data transformation operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add calculated columns\n",
    "df_enriched = df_sales \\\n",
    "    .withColumn(\"UnitPrice\", round(col(\"Amount\") / col(\"Quantity\"), 2)) \\\n",
    "    .withColumn(\"Year\", year(col(\"OrderDate\"))) \\\n",
    "    .withColumn(\"Month\", month(col(\"OrderDate\"))) \\\n",
    "    .withColumn(\"Quarter\", quarter(col(\"OrderDate\")))\n",
    "\n",
    "display(df_enriched.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregations\n",
    "monthly_summary = df_enriched \\\n",
    "    .groupBy(\"Year\", \"Month\", \"Region\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"OrderCount\"),\n",
    "        sum(\"Quantity\").alias(\"TotalQuantity\"),\n",
    "        round(sum(\"Amount\"), 2).alias(\"TotalRevenue\"),\n",
    "        round(avg(\"Amount\"), 2).alias(\"AvgOrderValue\")\n",
    "    ) \\\n",
    "    .orderBy(\"Year\", \"Month\", \"Region\")\n",
    "\n",
    "display(monthly_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Working with OneLake Shortcuts\n",
    "\n",
    "OneLake shortcuts allow you to reference data from external sources without copying it:\n",
    "\n",
    "### Types of Shortcuts:\n",
    "- **OneLake shortcuts**: Reference data in other OneLake locations\n",
    "- **ADLS Gen2 shortcuts**: Reference Azure Data Lake Storage\n",
    "- **S3 shortcuts**: Reference AWS S3 buckets\n",
    "\n",
    "Shortcuts are created through the Fabric UI:\n",
    "1. Navigate to your Lakehouse\n",
    "2. Right-click on Files or Tables\n",
    "3. Select \"New shortcut\"\n",
    "4. Choose the source type and configure connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once shortcuts are created, they appear as regular folders/tables\n",
    "# Example: reading from a shortcut\n",
    "# df_shortcut = spark.read.format(\"delta\").load(\"Tables/external_data_shortcut\")\n",
    "# display(df_shortcut.limit(10))\n",
    "\n",
    "print(\"Shortcuts provide seamless access to external data without data movement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Delta Lake Features\n",
    "\n",
    "OneLake uses Delta Lake format which provides advanced capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Travel - Query historical versions of data\n",
    "# df_version = spark.read.format(\"delta\") \\\n",
    "#     .option(\"versionAsOf\", 0) \\\n",
    "#     .load(f\"Tables/{delta_table_name}\")\n",
    "# display(df_version)\n",
    "\n",
    "print(\"Delta Lake supports time travel to query historical data versions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get table history\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# dt = DeltaTable.forPath(spark, f\"Tables/{delta_table_name}\")\n",
    "# history_df = dt.history()\n",
    "# display(history_df)\n",
    "\n",
    "print(\"Delta tables maintain complete history of all operations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Best Practices for OneLake\n",
    "\n",
    "### Storage Best Practices:\n",
    "1. **Use Delta tables** for structured data\n",
    "2. **Partition large tables** by frequently filtered columns (e.g., date)\n",
    "3. **Use appropriate file sizes** (128MB-1GB per file)\n",
    "4. **Leverage shortcuts** to avoid data duplication\n",
    "\n",
    "### Performance Best Practices:\n",
    "1. **Use Z-ORDER** for multi-dimensional clustering\n",
    "2. **Optimize file layout** with OPTIMIZE command\n",
    "3. **Cache frequently accessed data**\n",
    "4. **Use broadcast joins** for small dimension tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Optimize Delta table\n",
    "# spark.sql(f\"OPTIMIZE delta.`Tables/{delta_table_name}`\")\n",
    "# spark.sql(f\"OPTIMIZE delta.`Tables/{delta_table_name}` ZORDER BY (Region, Product)\")\n",
    "\n",
    "print(\"OPTIMIZE command compacts small files and improves query performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Data Governance in OneLake\n",
    "\n",
    "OneLake integrates with Microsoft Purview for comprehensive data governance:\n",
    "\n",
    "- **Data lineage**: Track data flow across your organization\n",
    "- **Data classification**: Automatically classify sensitive data\n",
    "- **Access control**: Manage permissions at workspace and item level\n",
    "- **Data discovery**: Find and understand available datasets\n",
    "\n",
    "These features are configured through the Fabric portal and Purview governance center."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "- ✅ OneLake architecture and benefits\n",
    "- ✅ Reading and writing data in multiple formats\n",
    "- ✅ Creating and querying Delta tables\n",
    "- ✅ Data transformations and aggregations\n",
    "- ✅ OneLake shortcuts for external data\n",
    "- ✅ Delta Lake advanced features\n",
    "- ✅ Best practices for storage and performance\n",
    "- ✅ Data governance capabilities\n",
    "\n",
    "## Next Steps\n",
    "- Explore Real-Time Intelligence for streaming data\n",
    "- Learn about Power BI semantic models\n",
    "- Implement data pipelines for orchestration\n",
    "- Set up CI/CD for your Fabric solutions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
