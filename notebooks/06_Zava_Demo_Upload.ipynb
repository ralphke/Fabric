{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zava Demo Data Upload to Fabric Lakehouse\n",
    "\n",
    "This notebook demonstrates how to upload the Zava demo sample parquet files to a Microsoft Fabric Lakehouse. The Zava demo contains e-commerce sample data including customers, products, orders, order items, and sales performance metrics.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- How to read parquet files from a local directory\n",
    "- How to upload data to the Fabric Lakehouse Files section\n",
    "- How to create Delta tables from parquet files\n",
    "- How to verify and query the uploaded data\n",
    "- Best practices for organizing data in OneLake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "To run this notebook, you need:\n",
    "- A Microsoft Fabric workspace\n",
    "- A Lakehouse created in your workspace\n",
    "- The zava demo parquet files from the `sample-data/zava-demo/` directory\n",
    "- Appropriate permissions to read/write data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "First, let's import the necessary libraries and verify our Spark environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import os\n",
    "\n",
    "# Display Spark configuration\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Application Name: {spark.sparkContext.appName}\")\n",
    "print(\"\\nEnvironment ready for data upload!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Source Data Location\n",
    "\n",
    "The Zava demo data consists of 5 parquet files representing a typical e-commerce database:\n",
    "- **customers.parquet**: 100 customer records\n",
    "- **products.parquet**: 50 product records\n",
    "- **orders.parquet**: 500 order records\n",
    "- **order_items.parquet**: 1000 order item records\n",
    "- **sales_performance.parquet**: 84 aggregated sales metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the source directory containing the parquet files\n",
    "# Note: In Fabric, you would typically upload these files first or reference them from a Git repo\n",
    "source_dir = \"/lakehouse/default/Files/sample-data/zava-demo/\"\n",
    "\n",
    "# Alternative: If running locally with the repo, use:\n",
    "# source_dir = \"../sample-data/zava-demo/\"\n",
    "\n",
    "# Define the files we want to upload\n",
    "files_to_upload = [\n",
    "    \"customers.parquet\",\n",
    "    \"products.parquet\",\n",
    "    \"orders.parquet\",\n",
    "    \"order_items.parquet\",\n",
    "    \"sales_performance.parquet\"\n",
    "]\n",
    "\n",
    "print(f\"Source directory: {source_dir}\")\n",
    "print(f\"Files to upload: {len(files_to_upload)}\")\n",
    "for f in files_to_upload:\n",
    "    print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Read and Explore the Parquet Files\n",
    "\n",
    "Let's read each parquet file and examine its schema and sample data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Customers Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read customers parquet file\n",
    "customers_path = os.path.join(source_dir, \"customers.parquet\")\n",
    "df_customers = spark.read.parquet(customers_path)\n",
    "\n",
    "print(\"Customers Schema:\")\n",
    "df_customers.printSchema()\n",
    "\n",
    "print(f\"\\nTotal Customers: {df_customers.count()}\")\n",
    "print(\"\\nSample Data:\")\n",
    "display(df_customers.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Products Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read products parquet file\n",
    "products_path = os.path.join(source_dir, \"products.parquet\")\n",
    "df_products = spark.read.parquet(products_path)\n",
    "\n",
    "print(\"Products Schema:\")\n",
    "df_products.printSchema()\n",
    "\n",
    "print(f\"\\nTotal Products: {df_products.count()}\")\n",
    "print(\"\\nSample Data:\")\n",
    "display(df_products.limit(5))\n",
    "\n",
    "print(\"\\nProduct Categories:\")\n",
    "display(df_products.groupBy(\"category\").count().orderBy(\"category\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Orders Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read orders parquet file\n",
    "orders_path = os.path.join(source_dir, \"orders.parquet\")\n",
    "df_orders = spark.read.parquet(orders_path)\n",
    "\n",
    "print(\"Orders Schema:\")\n",
    "df_orders.printSchema()\n",
    "\n",
    "print(f\"\\nTotal Orders: {df_orders.count()}\")\n",
    "print(\"\\nSample Data:\")\n",
    "display(df_orders.limit(5))\n",
    "\n",
    "print(\"\\nOrder Status Distribution:\")\n",
    "display(df_orders.groupBy(\"order_status\").count().orderBy(\"order_status\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Order Items Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read order_items parquet file\n",
    "order_items_path = os.path.join(source_dir, \"order_items.parquet\")\n",
    "df_order_items = spark.read.parquet(order_items_path)\n",
    "\n",
    "print(\"Order Items Schema:\")\n",
    "df_order_items.printSchema()\n",
    "\n",
    "print(f\"\\nTotal Order Items: {df_order_items.count()}\")\n",
    "print(\"\\nSample Data:\")\n",
    "display(df_order_items.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Sales Performance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read sales_performance parquet file\n",
    "sales_performance_path = os.path.join(source_dir, \"sales_performance.parquet\")\n",
    "df_sales_performance = spark.read.parquet(sales_performance_path)\n",
    "\n",
    "print(\"Sales Performance Schema:\")\n",
    "df_sales_performance.printSchema()\n",
    "\n",
    "print(f\"\\nTotal Records: {df_sales_performance.count()}\")\n",
    "print(\"\\nSample Data:\")\n",
    "display(df_sales_performance.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Upload Data to Lakehouse as Delta Tables\n",
    "\n",
    "Now let's write the data as Delta tables in the Lakehouse. Delta format provides:\n",
    "- ACID transactions\n",
    "- Time travel capabilities\n",
    "- Schema evolution\n",
    "- Optimal query performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Create Customers Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write customers data as Delta table\n",
    "table_name = \"zava_customers\"\n",
    "df_customers.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(f\"Tables/{table_name}\")\n",
    "\n",
    "print(f\"✓ Created Delta table: {table_name}\")\n",
    "print(f\"  Records: {df_customers.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Create Products Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write products data as Delta table\n",
    "table_name = \"zava_products\"\n",
    "df_products.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(f\"Tables/{table_name}\")\n",
    "\n",
    "print(f\"✓ Created Delta table: {table_name}\")\n",
    "print(f\"  Records: {df_products.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Create Orders Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write orders data as Delta table\n",
    "table_name = \"zava_orders\"\n",
    "df_orders.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(f\"Tables/{table_name}\")\n",
    "\n",
    "print(f\"✓ Created Delta table: {table_name}\")\n",
    "print(f\"  Records: {df_orders.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Create Order Items Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write order_items data as Delta table\n",
    "table_name = \"zava_order_items\"\n",
    "df_order_items.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(f\"Tables/{table_name}\")\n",
    "\n",
    "print(f\"✓ Created Delta table: {table_name}\")\n",
    "print(f\"  Records: {df_order_items.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Create Sales Performance Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write sales_performance data as Delta table\n",
    "table_name = \"zava_sales_performance\"\n",
    "df_sales_performance.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(f\"Tables/{table_name}\")\n",
    "\n",
    "print(f\"✓ Created Delta table: {table_name}\")\n",
    "print(f\"  Records: {df_sales_performance.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verify Uploaded Data\n",
    "\n",
    "Let's verify that all tables were created successfully and contain the expected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary of all uploaded tables\n",
    "tables = [\n",
    "    (\"zava_customers\", df_customers),\n",
    "    (\"zava_products\", df_products),\n",
    "    (\"zava_orders\", df_orders),\n",
    "    (\"zava_order_items\", df_order_items),\n",
    "    (\"zava_sales_performance\", df_sales_performance)\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"UPLOAD SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for table_name, df in tables:\n",
    "    count = df.count()\n",
    "    print(f\"✓ {table_name:30s} | {count:6d} records\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nAll Zava demo data has been successfully uploaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sample Queries and Analytics\n",
    "\n",
    "Now that the data is in the lakehouse, let's run some sample queries to demonstrate the data relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Top 10 Customers by Total Spending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query: Top 10 customers by total spending\n",
    "top_customers = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        c.customer_id,\n",
    "        c.first_name,\n",
    "        c.last_name,\n",
    "        c.email,\n",
    "        c.city,\n",
    "        c.state,\n",
    "        SUM(o.total_amount) as total_spent,\n",
    "        COUNT(DISTINCT o.order_id) as order_count\n",
    "    FROM delta.`Tables/zava_customers` c\n",
    "    JOIN delta.`Tables/zava_orders` o ON c.customer_id = o.customer_id\n",
    "    GROUP BY c.customer_id, c.first_name, c.last_name, c.email, c.city, c.state\n",
    "    ORDER BY total_spent DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "print(\"Top 10 Customers by Total Spending:\")\n",
    "display(top_customers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Sales by Product Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query: Sales by product category\n",
    "sales_by_category = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        p.category,\n",
    "        COUNT(DISTINCT o.order_id) as order_count,\n",
    "        SUM(oi.quantity) as total_units_sold,\n",
    "        ROUND(SUM(oi.quantity * oi.unit_price * (1 - oi.discount)), 2) as total_revenue,\n",
    "        ROUND(AVG(oi.unit_price), 2) as avg_unit_price\n",
    "    FROM delta.`Tables/zava_products` p\n",
    "    JOIN delta.`Tables/zava_order_items` oi ON p.product_id = oi.product_id\n",
    "    JOIN delta.`Tables/zava_orders` o ON oi.order_id = o.order_id\n",
    "    GROUP BY p.category\n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"Sales by Product Category:\")\n",
    "display(sales_by_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Monthly Order Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query: Monthly order trends\n",
    "monthly_trends = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        SUBSTRING(order_date, 1, 7) as year_month,\n",
    "        COUNT(*) as order_count,\n",
    "        ROUND(SUM(total_amount), 2) as total_revenue,\n",
    "        ROUND(AVG(total_amount), 2) as avg_order_value\n",
    "    FROM delta.`Tables/zava_orders`\n",
    "    GROUP BY SUBSTRING(order_date, 1, 7)\n",
    "    ORDER BY year_month\n",
    "\"\"\")\n",
    "\n",
    "print(\"Monthly Order Trends:\")\n",
    "display(monthly_trends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Top 10 Products by Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query: Top 10 products by revenue\n",
    "top_products = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        p.product_id,\n",
    "        p.product_name,\n",
    "        p.category,\n",
    "        p.price,\n",
    "        SUM(oi.quantity) as units_sold,\n",
    "        ROUND(SUM(oi.quantity * oi.unit_price * (1 - oi.discount)), 2) as total_revenue\n",
    "    FROM delta.`Tables/zava_products` p\n",
    "    JOIN delta.`Tables/zava_order_items` oi ON p.product_id = oi.product_id\n",
    "    GROUP BY p.product_id, p.product_name, p.category, p.price\n",
    "    ORDER BY total_revenue DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "print(\"Top 10 Products by Revenue:\")\n",
    "display(top_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Order Status Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query: Order status distribution with value\n",
    "status_distribution = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        order_status,\n",
    "        COUNT(*) as order_count,\n",
    "        ROUND(SUM(total_amount), 2) as total_value,\n",
    "        ROUND(AVG(total_amount), 2) as avg_order_value\n",
    "    FROM delta.`Tables/zava_orders`\n",
    "    GROUP BY order_status\n",
    "    ORDER BY order_count DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"Order Status Distribution:\")\n",
    "display(status_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Relationships and Integrity\n",
    "\n",
    "Let's verify the referential integrity of our data by checking the relationships between tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify referential integrity\n",
    "print(\"Data Relationship Verification:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check customers referenced in orders\n",
    "customers_in_orders = spark.sql(\"\"\"\n",
    "    SELECT COUNT(DISTINCT customer_id) as count\n",
    "    FROM delta.`Tables/zava_orders`\n",
    "\"\"\").collect()[0]['count']\n",
    "\n",
    "total_customers = df_customers.count()\n",
    "print(f\"Customers: {total_customers} total, {customers_in_orders} have orders\")\n",
    "\n",
    "# Check products referenced in order_items\n",
    "products_in_orders = spark.sql(\"\"\"\n",
    "    SELECT COUNT(DISTINCT product_id) as count\n",
    "    FROM delta.`Tables/zava_order_items`\n",
    "\"\"\").collect()[0]['count']\n",
    "\n",
    "total_products = df_products.count()\n",
    "print(f\"Products: {total_products} total, {products_in_orders} have been ordered\")\n",
    "\n",
    "# Check orders with items\n",
    "orders_with_items = spark.sql(\"\"\"\n",
    "    SELECT COUNT(DISTINCT order_id) as count\n",
    "    FROM delta.`Tables/zava_order_items`\n",
    "\"\"\").collect()[0]['count']\n",
    "\n",
    "total_orders = df_orders.count()\n",
    "print(f\"Orders: {total_orders} total, {orders_with_items} have items\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n✓ Data relationships verified successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Copy Files to Lakehouse Files Section (Optional)\n",
    "\n",
    "In addition to creating Delta tables, you can also copy the original parquet files to the Files section of your lakehouse for additional flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy parquet files to lakehouse Files section\n",
    "target_dir = \"Files/zava-demo/\"\n",
    "\n",
    "print(\"Copying parquet files to Files section...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for file_name in files_to_upload:\n",
    "    source_path = os.path.join(source_dir, file_name)\n",
    "    target_path = os.path.join(target_dir, file_name)\n",
    "    \n",
    "    # Read and write to copy the file\n",
    "    df = spark.read.parquet(source_path)\n",
    "    df.write.mode(\"overwrite\").parquet(target_path)\n",
    "    \n",
    "    print(f\"✓ Copied {file_name} to {target_path}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nAll files copied successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Best Practices and Next Steps\n",
    "\n",
    "### Best Practices for Lakehouse Data Organization:\n",
    "\n",
    "1. **Use Delta tables** for structured data that requires ACID transactions\n",
    "2. **Organize by domain** - group related tables together (e.g., zava_* prefix)\n",
    "3. **Implement partitioning** for large tables by date or other frequently filtered columns\n",
    "4. **Add table descriptions** and metadata for documentation\n",
    "5. **Set up data quality checks** to validate data integrity\n",
    "6. **Configure retention policies** for Delta table history\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Build semantic models** on top of these tables for Power BI reporting\n",
    "2. **Create SQL views** for common queries and business logic\n",
    "3. **Set up data pipelines** for incremental data loads\n",
    "4. **Implement row-level security** if needed for data governance\n",
    "5. **Create dashboards** to visualize sales trends and customer behavior\n",
    "6. **Explore advanced analytics** using the data for forecasting and predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "- ✅ Read parquet files from the zava-demo sample data\n",
    "- ✅ Uploaded 5 tables to the Fabric Lakehouse as Delta tables\n",
    "- ✅ Verified data integrity and relationships\n",
    "- ✅ Demonstrated sample queries for analytics\n",
    "- ✅ Copied original parquet files to the Files section\n",
    "\n",
    "The Zava demo data is now ready for use in:\n",
    "- Power BI reports and dashboards\n",
    "- Data science and machine learning experiments\n",
    "- Data pipeline development and testing\n",
    "- Learning and training scenarios\n",
    "\n",
    "**Total records uploaded:** 1,734 records across 5 tables\n",
    "- Customers: 100\n",
    "- Products: 50\n",
    "- Orders: 500\n",
    "- Order Items: 1,000\n",
    "- Sales Performance: 84"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
